2019-07-22 23:08:52 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: lianjia)
2019-07-22 23:08:52 [scrapy.utils.log] INFO: Versions: lxml 4.3.3.0, libxml2 2.9.5, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 19.2.0, Python 3.7.3 (v3.7.3:ef4ec6ed12, Mar 25 2019, 22:22:05) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Windows-10-10.0.17763-SP0
2019-07-22 23:08:52 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'lianjia', 'CONCURRENT_REQUESTS': 128, 'DOWNLOAD_DELAY': 0.01, 'FEED_EXPORT_ENCODING': 'utf-8', 'LOG_FILE': 'mySpider.log', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'lianjia.spiders', 'SPIDER_MODULES': ['lianjia.spiders']}
2019-07-22 23:08:52 [scrapy.extensions.telnet] INFO: Telnet Password: 626b14c5dbe072af
2019-07-22 23:08:52 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2019-07-22 23:08:52 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2019-07-22 23:08:52 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2019-07-22 23:08:52 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2019-07-22 23:08:52 [scrapy.core.engine] INFO: Spider opened
2019-07-22 23:08:52 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2019-07-22 23:08:52 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2019-07-22 23:09:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://nj.lianjia.com/ershoufang/liuhe/bp500ep100000/> (referer: None)
Traceback (most recent call last):
  File "c:\users\crisani\appdata\local\programs\python\python37\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "c:\users\crisani\appdata\local\programs\python\python37\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "c:\users\crisani\appdata\local\programs\python\python37\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "c:\users\crisani\appdata\local\programs\python\python37\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "c:\users\crisani\appdata\local\programs\python\python37\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "K:\Backup\代码\GitRepository\house_project\Scrapy\lianjia\lianjia\spiders\house_spider.py", line 282, in parse
    yield self.proxy.insert_house_price_item(items)
  File "K:\Backup\代码\GitRepository\house_project\Scrapy\lianjia\lianjia\mongo\mongo_tools.py", line 39, in insert_house_price_item
    self.db[house_price_item_set].insert(items)
  File "c:\users\crisani\appdata\local\programs\python\python37\lib\site-packages\pymongo\collection.py", line 3195, in insert
    check_keys, manipulate, write_concern)
  File "c:\users\crisani\appdata\local\programs\python\python37\lib\site-packages\pymongo\collection.py", line 648, in _insert
    blk.execute(write_concern, session=session)
  File "c:\users\crisani\appdata\local\programs\python\python37\lib\site-packages\pymongo\bulk.py", line 504, in execute
    raise InvalidOperation('No operations to execute')
pymongo.errors.InvalidOperation: No operations to execute
2019-07-22 23:09:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sh.lianjia.com/xiaoqu/> (referer: https://sh.lianjia.com/ershoufang/shanghaizhoubian/)
Traceback (most recent call last):
  File "c:\users\crisani\appdata\local\programs\python\python37\lib\site-packages\twisted\internet\defer.py", line 654, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "K:\Backup\代码\GitRepository\house_project\Scrapy\lianjia\lianjia\spiders\house_spider.py", line 307, in parse_detail_community
    self.proxy.insert_community_item(item)
  File "K:\Backup\代码\GitRepository\house_project\Scrapy\lianjia\lianjia\mongo\mongo_tools.py", line 36, in insert_community_item
    self.db[community_item_set].insert(items)
  File "c:\users\crisani\appdata\local\programs\python\python37\lib\site-packages\pymongo\collection.py", line 3195, in insert
    check_keys, manipulate, write_concern)
  File "c:\users\crisani\appdata\local\programs\python\python37\lib\site-packages\pymongo\collection.py", line 614, in _insert
    bypass_doc_val, session)
  File "c:\users\crisani\appdata\local\programs\python\python37\lib\site-packages\pymongo\collection.py", line 602, in _insert_one
    acknowledged, _insert_command, session)
  File "c:\users\crisani\appdata\local\programs\python\python37\lib\site-packages\pymongo\mongo_client.py", line 1280, in _retryable_write
    return self._retry_with_session(retryable, func, s, None)
  File "c:\users\crisani\appdata\local\programs\python\python37\lib\site-packages\pymongo\mongo_client.py", line 1233, in _retry_with_session
    return func(session, sock_info, retryable)
  File "c:\users\crisani\appdata\local\programs\python\python37\lib\site-packages\pymongo\collection.py", line 599, in _insert_command
    _check_write_command_response(result)
  File "c:\users\crisani\appdata\local\programs\python\python37\lib\site-packages\pymongo\helpers.py", line 217, in _check_write_command_response
    _raise_last_write_error(write_errors)
  File "c:\users\crisani\appdata\local\programs\python\python37\lib\site-packages\pymongo\helpers.py", line 198, in _raise_last_write_error
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
pymongo.errors.DuplicateKeyError: E11000 duplicate key error collection: house_db.community_item_set index: community_id_1 dup key: { : "" }
2019-07-22 23:09:49 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2019-07-22 23:09:49 [scrapy.core.engine] INFO: Closing spider (shutdown)
2019-07-22 23:09:52 [scrapy.extensions.logstats] INFO: Crawled 1032 pages (at 1032 pages/min), scraped 0 items (at 0 items/min)
2019-07-22 23:09:53 [scrapy.crawler] INFO: Received SIGINT twice, forcing unclean shutdown
2019-07-22 23:09:53 [scrapy.core.downloader.handlers.http11] WARNING: Got data loss in https://hz.lianjia.com/ershoufang/103105117210.html. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
